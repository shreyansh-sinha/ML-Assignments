# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vMTQgAR7HFfloqo2eUrkXAqZT7QchSVO
"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import os

! unzip 20news-19997.zip

my_path = "20news-19997/20_newsgroups"

# finds all the folders in directory

from os import listdir
folders = [f for f in listdir(my_path)]

folders

files = []
from os.path import isfile, join
for folder_name in folders:
  folder_path = join(my_path, folder_name)
  files.append([f for f in listdir(folder_path)])

pathname_list = []
for fo in range(len(folders)):
  for fi in files[fo]:
    pathname_list.append(join(my_path, join(folders[fo], fi)))
    
pathname_list

Y = []
for folder_name in folders:
  folder_path = join(my_path, folder_name)
  num_of_files = len(listdir(folder_path))
  for i in range(num_of_files):
    Y.append(folder_name)

Y

from sklearn.model_selection import train_test_split

import random

rand_doc_list = []
for i in range(3):
  rand_doc_list.append(random.choice(pathname_list))

rand_doc_list

import nltk
nltk.download('punkt')
'''
Splitting text into sentences creates a list where each sentence is an item
'''
# sentence splitter

def remove_metadata(lines):
  for i in range(len(lines)):
    if(lines[i] == '\n'):
      start = i+1
      break
  new_lines = lines[start:]
  return new_lines

log1 = open("20news-19997/20_newsgroups/talk.religion.misc/84552", "r")

sentences = []
for line in log1:
  sentences.append(line)

mod_sentence = remove_metadata(sentences)

# Function to convert   
def listToString(s):  
    
    # initialize an empty string 
    str1 = ""  
    
    # traverse in the string   
    for ele in s:  
        str1 += ele   
    
    # return string   
    return str1

sentence = listToString(mod_sentence)

# type(sentence)
text = "I do not like green eggs and ham. I do not like them Sam-I-am."

sentence = str(sentence)
# print(sentence)
a_list = (nltk.tokenize.sent_tokenize(sentence))

a_list

# Stopwords removal
'''
Typically, articles and pronouns are generally classified as stop words

The general strategy for determining a stop list is to sort the terms by collection frequency 
(the total number of times each term appears in the document collection), and then to take the most 
frequent terms, often hand-filtered for their semantic content relative to the domain of the documents 
being indexed, as a stop list , the members of which are then discarded during indexing
'''
import nltk
from nltk.corpus import stopwords

stop_words.append(stopwords.words('english'))

stop_words

#function to preprocess the words list to remove punctuations
import string
# from importlib import reload
# import sys
# reload(sys)
# sys.setdefaultencoding('utf-8')
def preprocess(words):
  #we'll make use of python's translate function,that maps one set of characters to another
  #we create an empty mapping table, the third argument allows us to list all of the characters 
  #to remove during the translation process
  
  #first we will try to filter out some  unnecessary data like tabs
  table = str.maketrans('', '', '\t')
  words = [word.translate(table) for word in words]
  
  punctuations = (string.punctuation).replace("'", "") 

  # the character: ' appears in a lot of stopwords and changes meaning of words if removed
  #hence it is removed from the list of symbols that are to be discarded from the documents
  trans_table = str.maketrans('', '', punctuations)
  stripped_words = [word.translate(trans_table) for word in words]
  
  #some white spaces may be added to the list of words, due to the translate function & nature of our documents
  #we remove them below
  words = [str for str in stripped_words if str]
  
  #some words are quoted in the documents & as we have not removed ' to maintain the integrity of some stopwords
  #we try to unquote such words below
  p_words = []
  for word in words:
      if (word[0] and word[len(word)-1] == "'"):
          word = word[1:len(word)-1]
      elif(word[0] == "'"):
          word = word[1:len(word)]
      else:
          word = word
      p_words.append(word)
  
  words = p_words.copy()
      
  #we will also remove just-numeric strings as they do not have any significant meaning in text classification
  words = [word for word in words if not word.isdigit()]
  
  #we will also remove single character strings
  words = [word for word in words if not len(word) == 1]
  
  #after removal of so many characters it may happen that some strings have become blank, we remove those
  words = [str for str in words if str]
  
  #we also normalize the cases of our words
  words = [word.lower() for word in words]
  
  #we try to remove words with only 2 characters
  words = [word for word in words if len(word) > 2]
  
  return words

#function to remove stopwords

def remove_stopwords(words):
  words = [word for word in words if not word in stop_words]
  return words

#function to convert a sentence into list of words

def tokenize_sentence(line):
  words = line[0:len(line)-1].strip().split(" ")
  words = preprocess(words)
  words = remove_stopwords(words)
    
  return words

#function to remove metadata

def remove_metadata(lines):
  for i in range(len(lines)):
    if(lines[i] == '\n'):
      start = i+1
      break
  new_lines = lines[start:]
  return new_lines

#function to convert a document into list of words

def tokenize(path):
  #load document as a list of lines
  f = open(path, 'r')
  text_lines = f.readlines()
  
  #removing the meta-data at the top of each document
  text_lines = remove_metadata(text_lines)
  
  #initiazing an array to hold all the words in a document
  doc_words = []
  
  #traverse over all the lines and tokenize each one with the help of helper function: tokenize_sentence
  for line in text_lines:
      doc_words.append(tokenize_sentence(line))

  return doc_words

#a simple helper function to convert a 2D array to 1D, without using numpy

def flatten(list):
  new_list = []
  for i in list:
    for j in i:
        new_list.append(j)
  return new_list

list_of_words = []

for document in rand_doc_list:
        list_of_words.append(flatten(tokenize(document)))

print(list_of_words)

print(len(list_of_words))

print(len(flatten(list_of_words)))


# from above lengths we observe that the code has been designed in such a way that the 2d list:
# list_of_words contains the vocabulary of each document file in the each of its rows, and
# collectively contains all the words we extract from the 20_newsgroups folder

import numpy as np
np_list_of_words = np.asarray(flatten(list_of_words))

np_list_of_words

#finding the number of unique words that we have extracted from the documents

words, counts = np.unique(np_list_of_words, return_counts=True)

counts
counts = counts.tolist()

type(counts)

#sorting the unique words according to their frequency

freq = []

for f in counts:
  if f not in freq:
    freq.append(f)

freq

f_o_w = []
n_o_w = []

for f in sorted(np.unique(counts), reverse=True):
  f_o_w.append(f)
  n_o_w.append(counts.count(f))

print(f_o_w)    
print(n_o_w)

# set of different frequencies and the no of words of
# that frequency

import matplotlib.pyplot as plt
y = f_o_w
x = n_o_w
plt.xlim(0,250)
plt.xlabel("No. of words")
plt.ylabel("Freq. of words")
plt.plot(x, y)
plt.grid()
plt.show()

rand_doc_list

import os
os.listdir(my_path)



# Tokenization

import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()


def remove_metadata(lines):
  for i in range(len(lines)):
    if(lines[i] == '\n'):
      start = i+1
      break
  new_lines = lines[start:]
  return new_lines

log = open("20news-19997/20_newsgroups/talk.religion.misc/84552", "r")

sentences = []
for line in log:
  sentences.append(line)

mod_sentence = remove_metadata(sentences)
mod_sentence

# Function to convert   
def listToString(s):  
    
    # initialize an empty string 
    str1 = ""  
    
    # traverse in the string   
    for ele in s:  
        str1 += ele   
    
    # return string   
    return str1

sentence = listToString(mod_sentence)

# sentence = "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun."

punctuations="?:!.,;"

sentence_words = nltk.word_tokenize(sentence)
for word in sentence_words:
  if word in punctuations:
    sentence_words.remove(word)

sentence1 = sentence
sentence_words

#A list of words to be stemmed
# stemmization

print("{0:20}{1:20}{2:20}".format("Word","Porter Stemmer","lancaster Stemmer"))

for word in np_list_of_words:
    print("{0:20}{1:20}{2:20}".format(word,porter.stem(word),lancaster.stem(word)))

# Stemmization on whole file

from nltk.tokenize import sent_tokenize, word_tokenize
def stemSentence(sentence):
    token_words=word_tokenize(sentence)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

log1 = open("20news-19997/20_newsgroups/talk.religion.misc/84552", "r")

for line in log1:
  x = stemSentence(line)
  print(x)

rand_doc_list

import nltk
nltk.download("wordnet")

# lemmatization
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()

print("{0:20}{1:20}".format("Word","Lemma"))

for word in sentence_words:
    print ("{0:20}{1:20}".format(word,wordnet_lemmatizer.lemmatize(word)))

list_of_words = []

documtent = "20news-19997/20_newsgroups/talk.politics.misc/178989"

list_of_words.append(flatten(tokenize(document)))

list_of_words

import numpy as np
np_list_of_words = np.asarray(flatten(list_of_words))

np_list_of_words

sentence1

# pos tagging
import nltk
token = nltk.word_tokenize(sentence1)
token

nltk.pos_tag(token)

# We can get more details about any POS tag using help funciton of NLTK as follows.
nltk.help.upenn_tagset("PRP$")

# chunking
#Define your grammar using regular expressions
grammar = ('''
    NP: {<DT>?<JJ>*<NN>} # NP
    ''')
chunkParser = nltk.RegexpParser(grammar)
tagged = nltk.pos_tag(nltk.word_tokenize(sentence))
tagged

tree = chunkParser.parse(tagged)
for subtree in tree.subtrees():
    print(subtree)

# words N-gram model

import nltk
import numpy as np
import random
import string

import bs4 as bs
import urllib.request
import re

ngrams = {}
words = 3

words_tokens = nltk.word_tokenize(sentence1)

for i in range(len(words_tokens) - words):
  seq = ' '.join(words_tokens[i:i+words])
  print(seq)
  if  seq not in ngrams.keys():
      ngrams[seq] = []
  ngrams[seq].append(words_tokens[i+words])

import nltk
nltk.download('names')

from sklearn.datasets import fetch_20newsgroups

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import names
from nltk.stem import WordNetLemmatizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def letters_only(astr):
  return astr.isalpha()

cv = CountVectorizer(stop_words = "english", max_features = 1000)

groups = fetch_20newsgroups()


groups
cleaned = []
all_names = set(names.words())
lemmatizer = WordNetLemmatizer()

for post in groups.data:
  cleaned.append(' '.join([
                           lemmatizer.lemmatize(word.lower())
                           for word in post.split()
                           if letters_only(word)
                           and word not in all_names]))
  
transformed = cv.fit_transform(cleaned)
km = KMeans(n_clusters = 5)
km.fit(transformed)
labels = groups.target

plt.scatter(labels, km.labels_)
plt.xlabel('Newsgroup')
plt.ylabel('Cluster')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups
import numpy as np
from sklearn.cluster import KMeans
from sklearn import metrics


categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
    'misc.forsale'
]

dataset = fetch_20newsgroups(subset='all', categories=categories,
                             shuffle=True, random_state=42)

# dataset

labels = dataset.target
len(labels)

dataset.target_names

true_k = 5

vectorizer = TfidfVectorizer(max_df=0.5,
                             min_df=2,
                             stop_words='english')

X = vectorizer.fit_transform(dataset.data)

km = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1)

km.fit(X)

order_centroids = km.cluster_centers_.argsort()[:, ::-1]   #centroids sorted position

terms = vectorizer.get_feature_names()

for i in range(true_k):
    print("cluster %d:" % i)
    for ind in order_centroids[i,:20]:
        print('%s' % terms[ind])
    print()

print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_))

print("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_))

from sklearn.datasets import fetch_20newsgroups
import pandas as pd
import matplotlib.pyplot as plt

#For taking a smaller group of data instead for testing
'''
categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
'''

# include 'remove=('headers','footers')' accordling to extract only certain parts of the text

newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers','footers'), random_state=42)

# to check list of categories (not required)
print(newsgroups_train.target_names)

print(newsgroups_train.data[0])

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(newsgroups_train.data)
X

# Run the following code to check inertia for different numbers of clusters.
# adjust range(x,y) to test number of clusters


inertia_list = []
for i in range(1,10):
    model = KMeans(n_clusters=i, init='k-means++', n_init = 1)
    model.fit(X)
    inertia_list.append(model.inertia_)
    print("#" + str(i) + ": " + str(model.inertia_))

plt.figure(figsize=(12,6))
plt.plot(range(1,10),inertia_list, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

